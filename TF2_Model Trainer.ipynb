{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h1><b>PROJECT HELPING HANDS Model Training</b></h1>\n","\n","Members:\n","1.   Renmar Balana................Leading Developer / Project Manager\n","2.   Xania Shane Oropesa......Full Stack Developer\n","3.   Hewey James Lita...........Tester / Front End Developer\n","4.   Jan Lance Borrero...........Frontend Developer, UI/UX designer\n","5.   Jiggy Brondial..................Data Gathering / Cleaning\n","\n","*BSCS 3-A (2022-2023)*\n","\n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **ROADMAP TO CREATING AN OBJECT DETECTION TFLITE MODEL**\n","\n","\n","* Collect the dataset of images and label them to get their xml files.\n","\n","* Install the TensorFlow Object Detection API.\n","\n","* Generate the TFRecord files required for training. (need generate_tfrecord.py script and csv files for this)\n","\n","* Edit the model pipeline config file and download the pre-trained model checkpoint.\n","\n","* Train and evaluate the model.\n","\n","* Export and convert the model into TFlite(TensorFlow Lite) format.\n","\n","* Deploy the TFlite model on Android / iOS / IoT devices.\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **0) Checking Prerequisites**\n","\n","At the time of testing, this notebook works with tensorflow version 2.12, and Python version 3.9.16. \n","If the current version of python does not work, try changing to alternative python versions by running the code below."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python --version\n","!sudo update-alternatives --config python3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!sudo apt install python3-pip"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **1) Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import glob\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","import tensorflow as tf\n","print(tf.__version__)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **2) Create *`customTF2`*, *`training`* and *`data`* folders in your google drive**\n","Create a folder named ***customTF2*** in your google drive.\n","\n","Create another folder named ***training*** inside the ***customTF2*** folder\n","(***training*** folder is where the checkpoints will be saved during training)\n","\n","Create another folder named ***data*** inside the ***customTF2*** folder."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **3) Create and upload your image files and xml files.**\n"," Create a folder named ***images*** for your custom dataset images and create another folder named ***annotations*** for its corresponding xml files.\n"," \n"," Next, create their zip files and upload them to the ***customTF2*** folder in your drive.\n","\n"," \n"," (Make sure all the image files have extension as \".jpg\" only.\n"," Other formats like \".png\" , \".jpeg\" or even \".JPG\" will give errors since the generate_tfrecord and xml_to_csv scripts here have only \".jpg\" in them)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Collect Images Dataset and label them to get their PASCAL_VOC XML annotations\n","For Datasets, you can check out my Dataset Sources at the bottom of this article in the credits section. You can use any software for labeling like the labelImg tool.\n","\n","Read this [article](https://viso.ai/computer-vision/labelimg-for-image-annotation/) to know more about collecting datasets and labeling process."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **4) Upload the *`generate_tfrecord.py`* file to the *`customTF2`* folder on your drive.**\n","\n","\n","You can find the generate_tfrecord.py file [here](https://github.com/techzizou/Train-Object-Detection-Model-TF-2.x)\n","\n","# **5) Mount drive and link your folder**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive\n","!ls /mydrive"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **6) Clone the tensorflow models git repository & Install TensorFlow Object Detection API**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# clone the tensorflow models on the colab cloud vm\n","!git clone --q https://github.com/tensorflow/models.git\n","\n","#navigate to /models/research folder to compile protos\n","%cd models/research\n","\n","# Compile protos.\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","# Install TensorFlow Object Detection API.\n","!cp object_detection/packages/tf2/setup.py .\n","!python -m pip install ."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **7) Test the model builder**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# testing the model builder to make sure all the required componenets are present.\n","!python object_detection/builders/model_builder_tf2_test.py"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **8) Navigate to /mydrive/customTF2/data/ and Unzip the *images.zip* and *annotations.zip* files into the *data* folder**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /mydrive/customTF2/data/\n","\n","# unzip the datasets and their contents so that they are now in /mydrive/customTF2/data/ folder\n","!unzip /mydrive/customTF2/images.zip -d .\n","!unzip /mydrive/customTF2/annotations.zip -d ."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **9) Create test_labels & train_labels**\n","Current working directory is /mydrive/customTF2/data/\n","\n","Divide annotations into test_labels(20%) and train_labels(80%)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /mydrive/customTF2/data/\n","\n","#creating two dir for training and testing\n","!mkdir test_labels train_labels\n","\n","# lists the files inside 'annotations' in a random order (not really random, by their hash value instead)\n","# Moves the first 271/1355 labels (20% of the labels) to the testing dir: `test_labels`\n","!ls annotations/* | sort -R | head -271 | xargs -I{} mv {} test_labels/\n","\n","\n","# Moves the rest of the labels ( 1084 labels ) to the training dir: `train_labels`\n","!ls annotations/* | xargs -I{} mv {} train_labels/"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Current working directory at this point\n","\n","![alt text](assets/cwd0.png \"Working Directory at this point\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **10) Create the CSV files and the \"label_map.pbtxt\" file**\n","\n","Current working directory is /mydrive/customTF2/data/\n","\n","Run this xml_to_csv script below to create ***test_labels.csv*** and ***train_labels.csv***\n","\n","This also creates the ***label_map.pbtxt*** file using the classes mentioned in the xml files. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /mydrive/customTF2/data/\n","\n","#adjusted from: https://github.com/datitran/raccoon_dataset\n","def xml_to_csv(path):\n","  classes_names = []\n","  xml_list = []\n","\n","  for xml_file in glob.glob(path + '/*.xml'):\n","    tree = ET.parse(xml_file)\n","    root = tree.getroot()\n","    for member in root.findall('object'):\n","      classes_names.append(member[0].text)\n","      print(root.find('filename').text)\n","      #print(int(root .find(\"object\").find(\"bndbox\").find(\"xmin\").text))\n","      #print(int(root.find('object/bndbox/xmin').text))\n","      value = (root.find('filename').text  ,\n","               int(root.find('size')[0].text),\n","               int(root.find('size')[1].text),\n","               member[0].text,\n","               int(root.find('object/bndbox/xmin').text),\n","               int(root.find('object/bndbox/ymin').text),\n","               int(root.find('object/bndbox/xmax').text),\n","               int(root.find('object/bndbox/ymax').text))\n","      xml_list.append(value)\n","  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n","  xml_df = pd.DataFrame(xml_list, columns=column_name) \n","  classes_names = list(set(classes_names))\n","  classes_names.sort()\n","  return xml_df, classes_names\n","\n","for label_path in ['train_labels', 'test_labels']:\n","  image_path = os.path.join(os.getcwd(), label_path)\n","  xml_df, classes = xml_to_csv(label_path)\n","  xml_df.to_csv(f'{label_path}.csv', index=None)\n","  print(f'Successfully converted {label_path} xml to csv.')\n","\n","label_map_path = os.path.join(\"label_map.pbtxt\")\n","pbtxt_content = \"\"\n","\n","for i, class_name in enumerate(classes):\n","    pbtxt_content = (\n","        pbtxt_content\n","        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n}}\\n\\n\".format(i + 1, class_name)\n","    )\n","pbtxt_content = pbtxt_content.strip()\n","with open(label_map_path, \"w\") as f:\n","    f.write(pbtxt_content)\n","    print('Successfully created label_map.pbtxt ')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Directory after step 10\n","\n","![alt text](assets/cwd2.png \"Directory after step 10\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **11) Create train.record & test.record files**\n","\n","Current working directory is /mydrive/customTF2/data/\n","\n","Run the *generate_tfrecord.py* script to create *train.record* and *test.record* files"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /mydrive/customTF2/data/\n","\n","'''Usage:  \n","!python generate_tfrecord.py output.csv output_pb.txt /path/to/images output.tfrecords'''\n","\n","#For train.record\n","!python /mydrive/customTF2/generate_tfrecord.py train_labels.csv  label_map.pbtxt images/ train.record\n","\n","#For test.record\n","!python /mydrive/customTF2/generate_tfrecord.py test_labels.csv  label_map.pbtxt images/ test.record"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **12) Download pre-trained model checkpoint** \n","\n","Current working directory is /mydrive/customTF2/data/\n","\n","Download **ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz** into the ***data*** folder & unzip it.\n","\n","A list of detection checkpoints for tensorflow 2.x can be found [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /mydrive/customTF2/data/\n","\n","#Download the pre-trained model ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz into the data folder & unzip it.\n","\n","!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n","!tar -xzvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **13) Get the model pipeline config file, make changes to it and put it inside the *data* folder**\n","\n","Current working directory is /mydrive/customTF2/data/\n","\n","Download **ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config** from ***/content/models/research/object_detection/configs/tf2***. Make the required changes to it and upload it to the ***/mydrive/custom/data*** folder.\n","\n","**OR**\n","\n","Edit the config file from ***/content/models/research/object_detection/configs/tf2*** in colab and copy the edited config file to the ***/mydrive/customTF2/data*** folder.\n","\n","You can also find the pipeline config file inside the model checkpoint folder we just downloaded in the previous step.\n","\n","**You need to make the following changes:**\n","*   change ***num_classes*** to number of your classes.\n","*   change ***test.record*** path, ***train.record*** path & ***labelmap*** path to the paths where you have created these files (paths should be relative to your current working directory while training).\n","* change ***fine_tune_checkpoint*** to the path of the directory where the downloaded checkpoint from step 12 is. \n","* change ***fine_tune_checkpoint_type*** with value **classification** or **detection** depending on the type..\n","* change ***batch_size*** to any multiple of 8 depending upon the capability of your GPU.\n","(eg:- 24,128,...,512).Mine is set to 64. \n","* change ***num_steps*** to number of steps you want the detector to train. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#copy the edited config file from the configs/tf2 directory to the data/ folder in your drive\n","\n","!cp /content/models/research/object_detection/configs/tf2/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config /mydrive/customTF2/data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **14) Load Tensorboard**\n","Tensorboard allows us to visualize the performance of our model as it is being trained"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#load tensorboard\n","\n","%load_ext tensorboard\n","%tensorboard --logdir '/content/gdrive/MyDrive/customTF2/training'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **15) Train the model** \n","The most important part. Depending on the number of steps or epochs, the training may last any time between 30 minutes to 4 hours in our experience.\n","## Navigate to the ***object_detection*** folder in colab vm"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["%cd /content/models/research/object_detection"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 15 (a) Training using model_main_tf2.py\n","\n","Here **{PIPELINE_CONFIG_PATH}** points to the pipeline config and **{MODEL_DIR}** points to the directory in which training checkpoints and events will be written.\n","\n","For best results, you should stop the training when the loss is less than 0.1 if possible, else train the model until the loss does not show any significant change for a while. The ideal loss should be below 0.05 (Try to get the loss as low as possible without overfitting the model. Don’t go too high on training steps to try and lower the loss if the model has already converged viz. if it does not reduce loss significantly any further and takes a while to go down. )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Run the command below from the content/models/research/object_detection directory\n","\"\"\"\n","PIPELINE_CONFIG_PATH=path/to/pipeline.config\n","MODEL_DIR=path to training checkpoints directory\n","NUM_TRAIN_STEPS=50000\n","SAMPLE_1_OF_N_EVAL_EXAMPLES=1\n","\n","python model_main_tf2.py -- \\\n","  --model_dir=$MODEL_DIR --num_train_steps=$NUM_TRAIN_STEPS \\\n","  --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \\\n","  --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n","  --alsologtostderr\n","\"\"\"\n","\n","!python model_main_tf2.py --pipeline_config_path=/mydrive/customTF2/data/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config --model_dir=/mydrive/customTF2/training --alsologtostderr"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 15 (b) Evaluation using model_main_tf2.py\n","\n","You can run this in parallel by opening another colab notebook and running this command simultaneously along with the training command above (don't forget to mount drive, clone the TF git repo and install the TF2 object detection API there as well), or simply after the training is complete. This will give you validation loss, mAP, etc so you have a better idea of how your model is performing.\n","\n","Here **{CHECKPOINT_DIR}** points to the directory with checkpoints produced by the training job. Evaluation events are written to **{MODEL_DIR/eval}**. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Run the command below from the content/models/research/object_detection directory\n","\"\"\"\n","PIPELINE_CONFIG_PATH=path/to/pipeline.config\n","MODEL_DIR=path to training checkpoints directory\n","CHECKPOINT_DIR=${MODEL_DIR}\n","NUM_TRAIN_STEPS=50000\n","SAMPLE_1_OF_N_EVAL_EXAMPLES=1\n","\n","python model_main_tf2.py -- \\\n","  --model_dir=$MODEL_DIR --num_train_steps=$NUM_TRAIN_STEPS \\\n","  --checkpoint_dir=${CHECKPOINT_DIR} \\\n","  --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \\\n","  --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n","  --alsologtostderr\n","\"\"\"\n","\n","!python model_main_tf2.py --pipeline_config_path=/mydrive/customTF2/data/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config --model_dir=/mydrive/customTF2/training/ --checkpoint_dir=/mydrive/customTF2/training/ --alsologtostderr"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## RETRAINING THE MODEL ( in case you get disconnected )\n","\n","\n","If you get disconnected or lose your session on colab vm, you can start your training where you left off as the checkpoint is saved on your drive inside the ***training*** folder. To restart the training simply run **steps 1, 5, 6, 7, 14 and 15.**\n","\n","Note that since we have all the files required for training like the record files,our edited pipeline config file,the label_map file and the model checkpoint folder, we do not need to create these again. \n","\n","**The model_main_tf2.py script saves the checkpoint every 1000 steps.** The training automatically restarts from the last saved checkpoint itself.\n","\n","However, if you see that it doesn't restart training from the last checkpoint you can make 1 change in the pipeline config file. Change **fine_tune_checkpoint** to where your latest trained checkpoints have been written and have it point to the latest checkpoint as shown below:\n","\n","\n","``` \n","fine_tune_checkpoint: \"/mydrive/customTF2/training/ckpt-X\" (where ckpt-X is the latest checkpoint)\n","\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **16) Test your trained model**\n","## Export inference graph\n","\n","Current working directory is /content/models/research/object_detection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /content/models/research/object_detection\n","\n","##Export inference graph\n","!python exporter_main_v2.py --trained_checkpoint_dir=/mydrive/customTF2/training --pipeline_config_path=/content/gdrive/MyDrive/customTF2/data/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config --output_directory /mydrive/customTF2/data/inference_graph"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Test your trained Object Detection model on images\n","\n","Current working directory is /content/models/research/object_detection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Different font-type for labels text.(This step is optional)\n","!wget https://freefontsdownload.net/download/160187/arial.zip\n","!unzip arial.zip -d .\n","\n","%cd utils/\n","!sed -i \"s/font = ImageFont.truetype('arial.ttf', 24)/font = ImageFont.truetype('arial.ttf', 50)/\" visualization_utils.py\n","%cd .."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /content/models/research/object_detection\n","\n","#Loading the saved_model\n","import tensorflow as tf\n","import time\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","from PIL import Image\n","from google.colab.patches import cv2_imshow\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as viz_utils\n","\n","IMAGE_SIZE = (12, 8) # Output display size as you want\n","import matplotlib.pyplot as plt\n","PATH_TO_SAVED_MODEL=\"/mydrive/customTF2/data/inference_graph/saved_model\"\n","print('Loading model...', end='')\n","\n","# Load saved model and build the detection function\n","detect_fn=tf.saved_model.load(PATH_TO_SAVED_MODEL)\n","print('Done!')\n","\n","#Loading the label_map\n","category_index=label_map_util.create_category_index_from_labelmap(\"/mydrive/customTF2/data/label_map.pbtxt\",use_display_name=True)\n","#category_index=label_map_util.create_category_index_from_labelmap([path_to_label_map],use_display_name=True)\n","\n","def load_image_into_numpy_array(path):\n","\n","    return np.array(Image.open(path))\n","\n","image_path = \"/mydrive/asl/scisso41.jpg\"\n","#print('Running inference for {}... '.format(image_path), end='')\n","\n","image_np = load_image_into_numpy_array(image_path)\n","\n","# The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n","input_tensor = tf.convert_to_tensor(image_np)\n","# The model expects a batch of images, so add an axis with `tf.newaxis`.\n","input_tensor = input_tensor[tf.newaxis, ...]\n","\n","detections = detect_fn(input_tensor)\n","\n","# All outputs are batches tensors.\n","# Convert to numpy arrays, and take index [0] to remove the batch dimension.\n","# We're only interested in the first num_detections.\n","num_detections = int(detections.pop('num_detections'))\n","detections = {key: value[0, :num_detections].numpy()\n","              for key, value in detections.items()}\n","detections['num_detections'] = num_detections\n","\n","# detection_classes should be ints.\n","detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","\n","image_np_with_detections = image_np.copy()\n","\n","viz_utils.visualize_boxes_and_labels_on_image_array(\n","      image_np_with_detections,\n","      detections['detection_boxes'],\n","      detections['detection_classes'],\n","      detections['detection_scores'],\n","      category_index,\n","      use_normalized_coordinates=True,\n","      max_boxes_to_draw=200,\n","      min_score_thresh=.4, # Adjust this value to set the minimum probability boxes to be classified as True\n","      agnostic_mode=False)\n","%matplotlib inline\n","plt.figure(figsize=IMAGE_SIZE, dpi=200)\n","plt.axis(\"off\")\n","plt.imshow(image_np_with_detections)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","test = pd.read_csv(\"/mydrive/customTF2/data/test_labels.csv\")\n","test\n","\n","\n","###### Detect and Class_map functions\n","\n","def detect(image_path):\n","\n","    print('Running inference for {}... '.format(image_path), end='')\n","    image_np = load_image_into_numpy_array(image_path)\n","    input_tensor = tf.convert_to_tensor(image_np) # The input needs to be a tensor, convert it using tf.convert_to_tensor.\n","    input_tensor = input_tensor[tf.newaxis, ...]  # The model expects a batch of images, so add an axis with tf.newaxis.\n","    detections = detect_fn(input_tensor)\n","    num_detections = int(detections.pop('num_detections'))\n","    detections = {key: value[0, :num_detections].numpy()\n","                  for key, value in detections.items()}\n","    detections['num_detections'] = num_detections\n","    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","\n","    detected_class = category_index[ (detections['detection_classes'][0]) ]['name']\n","    class_index = detections['detection_classes'][0] -1\n","    #print('\\n' + detected_class)\n","    #print(class_index)\n","    return class_index\n","    \n","\n","\n","def class_map(class_name):\n","\n","  if class_name == \"Paper\":\n","      return 0\n","  if class_name == \"Rock\":\n","      return 1\n","  if class_name == \"Scissors\":\n","      return 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#CREATES A CONFUSION MATRIX, based on the files in test_labels.csv and compares with the highest confidence inferred classes\n","import numpy as np\n","\n","#Initialize variables\n","rows = len(test)\n","images_dir = \"/mydrive/customTF2/data/images/\"\n","confusion_matrix = np.zeros( (3,3), dtype=np.int32)\n","i=0\n","\n","#Iterate through all rows of test data\n","while i < rows:\n","   #print(test.filename[i])\n","   curr_image = images_dir + test.filename[i]\n","\n","   #perform the inference\n","   detected_index = detect(curr_image)\n","\n","   #Check if prediction is correct with actual label\n","   detected_class = category_index[detected_index+1]['name']\n","\n","   a = test['class'][i]\n","\n","   correct_index = class_map(test['class'][i])\n","\n","   confusion_matrix[detected_index][correct_index] += 1\n","   print(\"STEP\", i)\n","   i += 1\n","\n","\n","   #if (detected_class == test['class'][i]):\n","   #  print(\"CORRECT\")\n","   #elif ():\n","   #  confusion_matrix[detected_index][correct_index] += 1\n","   #else:\n","   #1 + 1\n","print(confusion_matrix)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Show the Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt     \n","\n","ax= plt.subplot()\n","sns.heatmap(confusion_matrix, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","\n","# labels, title and ticks\n","ax.set_xlabel('True labels');ax.set_ylabel('Predicted labels'); \n","ax.set_title('Confusion Matrix');\n","\n","ax.xaxis.set_ticklabels(['Paper', 'Rock', 'Scissors']); ax.yaxis.set_ticklabels(['Paper', 'Rock', 'Scissors']);"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# CONVERTING TRAINED SSD MODEL TO TFLITE MODEL \n","\n","# **17) INSTALL tf-nightly** \n","TFLite converter works better with tf-nightly.\n","\n","### **NOTE:** Once you install tf-nightly and restart runtime and if you lose all the local variables in your google colab session, run steps 5 & 6 again to mount the drive and install TensorFlow object detection api "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install tf-nightly==2.13.0.dev20230304"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **18) Export SSD TFLite graph**\n","\n","Current working directory is /content/models/research/object_detection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install keras-nightly==2.13.0.dev2023030408 protobuf==4.22.0 tb-nightly==2.13.0a20230304 tensorboard-data-server==0.7.0 tf-estimator-nightly==2.13.0.dev2023030409 tf-nightly==2.13.0.dev20230304 wrapt==1.14.1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install protobuf==3.20.3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /content/models/research/object_detection\n","!python export_tflite_graph_tf2.py --pipeline_config_path /content/gdrive/MyDrive/customTF2/data/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config --trained_checkpoint_dir /mydrive/customTF2/training --output_directory /mydrive/customTF2/data/tflite"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **19) Convert TF saved model to TFLite model**\n","\n","Current working directory is /mydrive/customTF2/data/\n","## Check input and output tensor names"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!saved_model_cli show --dir /mydrive/customTF2/data/tflite/saved_model --tag_set serve --all"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Converting to TFlite:\n","#### Using command-line tool `tflite_convert`- (Basic model conversion)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The default inference type is Floating-point.\n","%cd /mydrive/customTF2/data/\n","\n","!tflite_convert --saved_model_dir=tflite/saved_model --output_file=tflite/detect.tflite"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **20) Create TFLite metadata**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install tflite-support-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /mydrive/customTF2/data/\n","%cd tflite/\n","!mkdir tflite_with_metadata\n","%cd .."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Create a ***labelmap.txt*** file with the names of the classes written in each line inside the ***data*** folder. \n","\n","Finally run the following cell to create the ***detect.tflite*** model with metadata attached to it.\n","Current working directory is /mydrive/customTF2/data/"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /mydrive/customTF2/data/\n","\n","# Attach Metadata to TFLite\n","from tflite_support.metadata_writers import object_detector\n","from tflite_support.metadata_writers import writer_utils\n","from tflite_support import metadata\n","import flatbuffers\n","import os\n","from tensorflow_lite_support.metadata import metadata_schema_py_generated as _metadata_fb\n","from tensorflow_lite_support.metadata.python import metadata as _metadata\n","from tensorflow_lite_support.metadata.python.metadata_writers import metadata_info\n","from tensorflow_lite_support.metadata.python.metadata_writers import metadata_writer\n","from tensorflow_lite_support.metadata.python.metadata_writers import writer_utils\n","\n","ObjectDetectorWriter = object_detector.MetadataWriter\n","\n","_MODEL_PATH = \"/mydrive/customTF2/data/tflite/detect.tflite\"\n","_LABEL_FILE = \"/mydrive/customTF2/data/labelmap.txt\"\n","_SAVE_TO_PATH = \"/mydrive/customTF2/data/tflite/tflite_with_metadata/detect.tflite\"\n","\n","writer = ObjectDetectorWriter.create_for_inference(\n","    writer_utils.load_file(_MODEL_PATH), [127.5], [127.5], [_LABEL_FILE])\n","writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)\n","\n","# Verify the populated metadata and associated files.\n","displayer = metadata.MetadataDisplayer.with_model_file(_SAVE_TO_PATH)\n","print(\"Metadata populated:\")\n","print(displayer.get_metadata_json())\n","print(\"Associated file(s) populated:\")\n","print(displayer.get_packed_associated_file_list())\n","\n","model_meta = _metadata_fb.ModelMetadataT()\n","model_meta.name = \"SSD_Detector\"\n","model_meta.description = (\n","    \"Identify which of a known set of objects might be present and provide \"\n","    \"information about their positions within the given image or a video \"\n","    \"stream.\")\n","\n","# Creates input info.\n","input_meta = _metadata_fb.TensorMetadataT()\n","input_meta.name = \"image\"\n","input_meta.content = _metadata_fb.ContentT()\n","input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n","input_meta.content.contentProperties.colorSpace = (\n","    _metadata_fb.ColorSpaceType.RGB)\n","input_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.ImageProperties)\n","input_normalization = _metadata_fb.ProcessUnitT()\n","input_normalization.optionsType = (\n","    _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n","input_normalization.options = _metadata_fb.NormalizationOptionsT()\n","input_normalization.options.mean = [127.5]\n","input_normalization.options.std = [127.5]\n","input_meta.processUnits = [input_normalization]\n","input_stats = _metadata_fb.StatsT()\n","input_stats.max = [255]\n","input_stats.min = [0]\n","input_meta.stats = input_stats\n","\n","# Creates outputs info.\n","output_location_meta = _metadata_fb.TensorMetadataT()\n","output_location_meta.name = \"location\"\n","output_location_meta.description = \"The locations of the detected boxes.\"\n","output_location_meta.content = _metadata_fb.ContentT()\n","output_location_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.BoundingBoxProperties)\n","output_location_meta.content.contentProperties = (\n","    _metadata_fb.BoundingBoxPropertiesT())\n","output_location_meta.content.contentProperties.index = [1, 0, 3, 2]\n","output_location_meta.content.contentProperties.type = (\n","    _metadata_fb.BoundingBoxType.BOUNDARIES)\n","output_location_meta.content.contentProperties.coordinateType = (\n","    _metadata_fb.CoordinateType.RATIO)\n","output_location_meta.content.range = _metadata_fb.ValueRangeT()\n","output_location_meta.content.range.min = 2\n","output_location_meta.content.range.max = 2\n","\n","output_class_meta = _metadata_fb.TensorMetadataT()\n","output_class_meta.name = \"category\"\n","output_class_meta.description = \"The categories of the detected boxes.\"\n","output_class_meta.content = _metadata_fb.ContentT()\n","output_class_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.FeatureProperties)\n","output_class_meta.content.contentProperties = (\n","    _metadata_fb.FeaturePropertiesT())\n","output_class_meta.content.range = _metadata_fb.ValueRangeT()\n","output_class_meta.content.range.min = 2\n","output_class_meta.content.range.max = 2\n","label_file = _metadata_fb.AssociatedFileT()\n","label_file.name = os.path.basename(\"labelmap.txt\")\n","label_file.description = \"Label of objects that this model can recognize.\"\n","label_file.type = _metadata_fb.AssociatedFileType.TENSOR_VALUE_LABELS\n","output_class_meta.associatedFiles = [label_file]\n","\n","output_score_meta = _metadata_fb.TensorMetadataT()\n","output_score_meta.name = \"score\"\n","output_score_meta.description = \"The scores of the detected boxes.\"\n","output_score_meta.content = _metadata_fb.ContentT()\n","output_score_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.FeatureProperties)\n","output_score_meta.content.contentProperties = (\n","    _metadata_fb.FeaturePropertiesT())\n","output_score_meta.content.range = _metadata_fb.ValueRangeT()\n","output_score_meta.content.range.min = 2\n","output_score_meta.content.range.max = 2\n","\n","output_number_meta = _metadata_fb.TensorMetadataT()\n","output_number_meta.name = \"number of detections\"\n","output_number_meta.description = \"The number of the detected boxes.\"\n","output_number_meta.content = _metadata_fb.ContentT()\n","output_number_meta.content.contentPropertiesType = (\n","    _metadata_fb.ContentProperties.FeatureProperties)\n","output_number_meta.content.contentProperties = (\n","    _metadata_fb.FeaturePropertiesT())\n","\n","# Creates subgraph info.\n","group = _metadata_fb.TensorGroupT()\n","group.name = \"detection result\"\n","group.tensorNames = [\n","    output_location_meta.name, output_class_meta.name,\n","    output_score_meta.name\n","]\n","subgraph = _metadata_fb.SubGraphMetadataT()\n","subgraph.inputTensorMetadata = [input_meta]\n","subgraph.outputTensorMetadata = [\n","    output_location_meta, output_class_meta, output_score_meta,\n","    output_number_meta\n","]\n","subgraph.outputTensorGroups = [group]\n","model_meta.subgraphMetadata = [subgraph]\n","\n","b = flatbuffers.Builder(0)\n","b.Finish(\n","    model_meta.Pack(b),\n","    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n","metadata_buf = b.Output()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### OR this. Run only 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Attach Metadata to TFLite\n","from tflite_support.metadata_writers import object_detector\n","from tflite_support.metadata_writers import writer_utils\n","from tflite_support import metadata\n","import flatbuffers\n","import os\n","from tensorflow_lite_support.metadata import metadata_schema_py_generated as _metadata_fb\n","from tensorflow_lite_support.metadata.python import metadata as _metadata\n","from tensorflow_lite_support.metadata.python.metadata_writers import metadata_info\n","from tensorflow_lite_support.metadata.python.metadata_writers import metadata_writer\n","from tensorflow_lite_support.metadata.python.metadata_writers import writer_utils\n","\n","ObjectDetectorWriter = object_detector.MetadataWriter\n","_MODEL_PATH = \"/mydrive/customTF2/data/tflite/detect.tflite\"\n","# Task Library expects label files that are in the same format as the one below.\n","_LABEL_FILE = \"/mydrive/customTF2/data/labelmap.txt\"\n","_SAVE_TO_PATH = \"/mydrive/customTF2/data/tflite/tflite_with_metadata/detect.tflite\"\n","\n","# Normalization parameters is required when reprocessing the image. It is\n","# optional if the image pixel values are in range of [0, 255] and the input\n","# tensor is quantized to uint8. See the introduction for normalization and\n","# quantization parameters below for more details.\n","# https://www.tensorflow.org/lite/models/convert/metadata#normalization_and_quantization_parameters)\n","_INPUT_NORM_MEAN = 127.5\n","_INPUT_NORM_STD = 127.5\n","\n","# Create the metadata writer.\n","writer = ObjectDetectorWriter.create_for_inference(\n","    writer_utils.load_file(_MODEL_PATH), [_INPUT_NORM_MEAN], [_INPUT_NORM_STD],\n","    [_LABEL_FILE])\n","\n","# Verify the metadata generated by metadata writer.\n","print(writer.get_metadata_json())\n","\n","# Populate the metadata into the model.\n","writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### <u>**TROUBLESHOOTING:**</u> \n","If you get an error for _registerMatType cv2 above, this might be because of OpenCV version mismatches in Colab. Run `!pip list|grep opencv` to see the versions of OpenCV packages installed i.e. `opencv-python`, `opencv-contrib-python` & `opencv-python-headless`. The versions will be different which is causing this error. This error will go away soon when colab updates it supported versions. For now, you can fix this by simply uninstalling and installing OpenCV again. \n","\n","**Check versions:**\n","\n","!pip list|grep opencv\n","\n","\n","**Use the following 2 commands if only the opencv-python-headless is of different version**\n","\n","!pip uninstall opencv-python-headless --y\n","\n","!pip install opencv-python-headless==4.1.2.30\n","\n","\n","**Or use the following commands if other opencv packages are of different versions. Uninstall and install all with the same version.**\n","\n","!pip uninstall opencv-python --y\n","\n","!pip uninstall opencv-contrib-python --y\n","\n","!pip uninstall opencv-python-headless --y\n","\n","\n","!pip install opencv-python==4.5.4.60\n","\n","!pip install opencv-contrib-python==4.5.4.60\n","\n","!pip install opencv-python-headless==4.5.4.60"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1BV5VLNX5Ivc0jruOB01oHcuufxX0Y5fp","timestamp":1686026317148},{"file_id":"1dF4H3E2cSdirhOrrGh8d0pW5la9MKmoR","timestamp":1683790974598},{"file_id":"1vJzjx2UT3qTwpU6dpA2gHV4LjdhyZpoU","timestamp":1682051935215},{"file_id":"16KNzz9ez5wYFFKIgmtWb3qdXJlvov1yQ","timestamp":1680022478789},{"file_id":"13SPNsogGr8o4KQvBAxJBBCIonUx5Xad8","timestamp":1671921180995},{"file_id":"1IV6wvOXVJBoP2BlyU5XA33LTWAgkHX-y","timestamp":1636485394082}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
